# -*- coding: utf-8 -*-
"""BaseLine.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1hY8xfA78vjyyiz7EkYDQ56MeJkE282u7
"""

import time
import torch
import pickle
import warnings
import numpy as np
import pandas as pd
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import f1_score
from tqdm import tqdm_notebook
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from torchtext.data import Field, BucketIterator, TabularDataset, Iterator
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression


torch.manual_seed(0) 
torch.backends.cudnn.deterministic = True 
torch.backends.cudnn.benchmark = False 
np.random.seed(0)

pd.set_option('display.max_colwidth', -1) 
pd.set_option("display.max_rows", 1000)

device = torch.device("cuda:0")
torch.cuda.set_device(0)

warnings.filterwarnings('ignore')

"""# ____________________"""

data = pd.read_csv('/my-part-trpo2020/data_train.csv')

data_test = pd.read_csv('/my-part-trpo2020/data_test_.csv')

X_train = data.FULLNAME.values
gender_train = data.GENDER.values
vectorizer = CountVectorizer(analyzer='char', ngram_range=(3,3))
X_train = vectorizer.fit_transform(X_train)
gender_model = LogisticRegression()
gender_model.fit(X_train, gender_train)

X = vectorizer.transform(['Путин Владимир Владимирович'])
gender_model.predict(X)[0]

data.head()

def split_fio(s):
    return s.replace(' ', '#').replace('', ' ')[1:-1]

data.FULLNAME = data.FULLNAME.apply(split_fio)

data.head()

data = data[['FULLNAME', 'NATION']]

encoder = LabelEncoder()
data.NATION = encoder.fit_transform(data.NATION)

data.head()

len(set(data.NATION))

data.head()

with open('/my-part-trpo2020/label_enc.pkl', 'wb') as f:
    pickle.dump(encoder, f)

train, val = train_test_split(data, test_size=0.2, random_state=42)

train.to_csv('/my-part-trpo2020/train.csv', index=None)
val.to_csv('/my-part-trpo2020/test.csv', index=None)

data_test.FULLNAME = data_test.FULLNAME.apply(split_fio)

data_test.head()

data_test.to_csv('/my-part-trpo2020/comp_test.csv', index=None)

tokenize = lambda x: x.split(' ')

TEXT = Field(sequential=True, tokenize=tokenize, lower=True)
LABEL = Field(sequential=False, use_vocab=False, is_target=True)

nation_fields = [('FULLNAME', TEXT), ('NATION', LABEL)]

trn, vld = TabularDataset.splits(path='/my-part-trpo2020/',
                                 train='train.csv',
                                 validation="test.csv",
                                 format='csv',
                                 skip_header=True,
                                 fields=nation_fields)

TEXT.build_vocab(trn)

TEXT.vocab.freqs.most_common(10)

TEXT.vocab.stoi

batch_size = 256

train_iter, val_iter = BucketIterator.splits((trn, vld),
                                             batch_sizes=(batch_size, batch_size),
                                             device=device,
                                             sort_key=lambda x: len(x.FULLNAME),
                                             sort_within_batch=False,
                                             repeat=False)
class GlobalMaxPooling(nn.Module):
    def __init__(self, dim=-1):
        super(self.__class__, self).__init__()
        self.dim = dim
        
    def forward(self, x):
        return x.max(dim=self.dim)[0]


class SimpleBiLSTM(nn.Module):
    def __init__(self, inp_dim, hidden_dim=700, emb_dim=300, output_dim=188):
        super().__init__()
        self.embedding = nn.Embedding(inp_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=1, bidirectional=True)
        self.pool = GlobalMaxPooling(dim=0) 
        self.dropout = nn.Dropout(0.25, inplace=True)
        self.linear = nn.Linear(2 * hidden_dim, hidden_dim)
        self.relu = nn.ReLU(inplace=True)
        self.predictor = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, seq):
        # [seq_length, batch_size]
        x = self.embedding(seq) # [seq_length, batch_size, emb_dim]
        x, _ = self.lstm(self.embedding(seq)) # [seq_length, batch_size, num_dir * hid_size]
        x = self.pool(x) # [batch_size, num_dir * hid_size]
        x = self.dropout(x) # [batch_size, num_dir * hid_size]
        x = self.linear(x) # [batch_size, hid_size]
        x = self.relu(x) # [batch_size, hid_size]
        preds = self.predictor(x) # [batch_size, output_size]
        return preds

model = SimpleBiLSTM(inp_dim=len(TEXT.vocab))
model.cuda()

model.load_state_dict(torch.load('/my-part-trpo2020/best_model.pt'))
model.eval()

with open('/my-part-trpo2020/label_enc.pkl', 'rb') as f:
    encoder = pickle.load(f)

def predict(name, model=model):
  X = vectorizer.transform([name])
  name = name.replace(' ', '#').replace('', ' ').lower()[1: -1]
  name = [TEXT.tokenize(name)]
  inp = TEXT.numericalize(TEXT.pad(name)).cuda()
  preds = model(inp)
  pred = preds.max(1)[1].data
  gender_pred = gender_model.predict(X)[0]
  return encoder.inverse_transform(pred.data.cpu().numpy())[0], gender_pred

print('Type your Fullname') 
question = str(input()) 
print(predict(question))
